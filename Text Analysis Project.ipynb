{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74961f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import requests\n",
    "import openpyxl\n",
    "import syllables\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a04842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"Output Data Structure.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0690ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract article text from a URL\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            article_title = soup.find('h1').text.strip()  \n",
    "            article_text = \"\"\n",
    "            for paragraph in soup.find_all('p'):\n",
    "                article_text += paragraph.text.strip() + \"\\n\"\n",
    "            return article_title, article_text\n",
    "        else:\n",
    "            print(f\"Failed to retrieve content from {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    return None, None\n",
    "\n",
    "# Function to save article to text file\n",
    "def save_to_text_file(url_id, article_title, article_text):\n",
    "    if article_title and article_text:\n",
    "        file_name = f\"{url_id}.txt\"\n",
    "        with open(file_name, 'w', encoding='utf-8') as file:\n",
    "            file.write(f\"Title: {article_title}\\n\\n\")\n",
    "            file.write(article_text)\n",
    "        print(f\"Saved {file_name}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a7c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data \n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "input_excel_file = 'Input.xlsx'\n",
    "wb = openpyxl.load_workbook(input_excel_file)\n",
    "sheet = wb.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd966bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming URL_ID is in the first column (A) of the Excel file\n",
    "for row in sheet.iter_rows(min_row=2, values_only=True):\n",
    "    url_id, url = row[0], row[1]\n",
    "    article_title, article_text = extract_article_text(url)\n",
    "    save_to_text_file(url_id, article_title, article_text)\n",
    "\n",
    "# Load positive and negative words from files\n",
    "def load_words_from_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        return set(file.read().splitlines())\n",
    "\n",
    "positive_words = load_words_from_file('positive_words.txt')\n",
    "negative_words = load_words_from_file('negative_words.txt')\n",
    "\n",
    "# Load stop words from file\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "with open('combined_file.txt', 'r', encoding='utf-8') as file:\n",
    "    stop_words.update(file.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc85fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Excel file for output\n",
    "output_excel_file = 'Output.xlsx'\n",
    "output_wb = openpyxl.Workbook()\n",
    "output_sheet = output_wb.active\n",
    "\n",
    "# Create headers for the output sheet\n",
    "output_sheet.append([\"URL_ID\", \"URL\", \"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\", \"SUBJECTIVITY SCORE\",\n",
    "                     \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\",\n",
    "                     \"AVG NUMBER OF WORDS PER SENTENCE\", \"COMPLEX WORD COUNT\", \"WORD COUNT\", \"SYLLABLE PER WORD\",\n",
    "                     \"PERSONAL PRONOUNS\", \"AVG WORD LENGTH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c53e7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):      # Function to calculate the number of syllables in a word\n",
    "    return syllables.estimate(word)\n",
    "\n",
    "\n",
    "for row in sheet.iter_rows(min_row=2, values_only=True):\n",
    "    url_id, url = row[0], row[1]\n",
    "\n",
    "    \n",
    "    file_name = f\"{url_id}.txt\"    # Read the text file corresponding to the URL_ID\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        encodings_to_try = ['utf-8', 'latin-1', 'windows-1252']\n",
    "        text = None\n",
    "\n",
    "        for encoding in encodings_to_try:\n",
    "            try:\n",
    "                with open(file_name, 'r', encoding=encoding) as file:\n",
    "                    text = file.read()\n",
    "                break  \n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "\n",
    "        if text is None:\n",
    "            print(f\"Failed to read {file_name} with all encodings. Skipping...\")\n",
    "            continue  # Skip processing this file\n",
    "            \n",
    "        # Tokenize the text\n",
    "        words = word_tokenize(text)\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords, and convert to lowercase\n",
    "        clean_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "        \n",
    "        # Calculate positive and negative scores using custom word lists\n",
    "        positive_score = len([word for word in clean_words if word in positive_words])\n",
    "        negative_score = len([word for word in clean_words if word in negative_words])\n",
    "        \n",
    "        # Calculate polarity and subjectivity scores\n",
    "        blob = TextBlob(text)\n",
    "        polarity_score = blob.sentiment.polarity\n",
    "        subjectivity_score = blob.sentiment.subjectivity\n",
    "        \n",
    "        # Calculate average sentence length\n",
    "        avg_sentence_length = len(words) / len(sentences)\n",
    "        \n",
    "        # Calculate percentage of complex words\n",
    "        complex_words = [word for word in clean_words if count_syllables(word) > 2]\n",
    "        percentage_complex_words = (len(complex_words) / len(clean_words)) * 100\n",
    "        \n",
    "        # Calculate Fog Index\n",
    "        fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "        \n",
    "        # Calculate average number of words per sentence\n",
    "        avg_words_per_sentence = len(clean_words) / len(sentences)\n",
    "        \n",
    "        # Calculate complex word count\n",
    "        complex_word_count = len(complex_words)\n",
    "        \n",
    "        # Calculate word count\n",
    "        word_count = len(clean_words)\n",
    "        \n",
    "        # Calculate syllables per word\n",
    "        syllables_per_word = sum(count_syllables(word) for word in clean_words) / len(clean_words)\n",
    "        \n",
    "        # Calculate personal pronouns count\n",
    "        personal_pronouns_count = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.IGNORECASE))\n",
    "        \n",
    "        # Calculate average word length\n",
    "        avg_word_length = sum(len(word) for word in clean_words) / len(clean_words)\n",
    "        \n",
    "        # Append the results \n",
    "        output_sheet.append([url_id, url, positive_score, negative_score, polarity_score, subjectivity_score,\n",
    "                             avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence,\n",
    "                             complex_word_count, word_count, syllables_per_word, personal_pronouns_count,\n",
    "                             avg_word_length])\n",
    "\n",
    "# Save the output Excel file\n",
    "output_wb.save(output_excel_file)\n",
    "output_wb.close()\n",
    "\n",
    "# Close the input Excel file\n",
    "wb.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5fb740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
